{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's obtain the California Housing dataset, and dviding it into training and prediction sets. The training set itself will be split 80/20 into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joltml import Experiment, Xgboost, Sklearn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "# Logging is done implicitly\n",
    "dataset = fetch_california_housing(as_frame=True)[\"frame\"]\n",
    "dataset = dataset.dropna()\n",
    "training_set = dataset[:16000]\n",
    "prediction_set = dataset.iloc[16000:,:-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a joltml Experiment object to start our experiment. In this experiment, we add an Xgboost model via the joltml `Xgboost` wrapper class, apply regression and then predict using the trained model. Evaluation metrics are written in the file `experiment_1/jobid/fits_book.json` where `jobid` is a generated random string - unless you specify one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "experiment = Experiment(training_set,experiment_id='experiment_1')\n",
    "experiment.add_models([Xgboost()])\n",
    "experiment.regression(target_names=['MedHouseVal'],splits=0.2)\n",
    "# Write results and default evaluation metrics\n",
    "y = experiment.predict(prediction_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do the model addition, regression and prediction in just one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(training_set)\n",
    "y = experiment.add_models([Xgboost()]).regression(target_names=['MedHouseVal'], splits=[0.8,0.2]).predict(prediction_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the `Xgboost` wrapper, we could use the `sklearn` wrapper and specify `ElasticNet()` as the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "experiment = Experiment(training_set)\n",
    "y = experiment.add_models([Sklearn(ElasticNet())]).regression(target_names=['MedHouseVal'],splits=[0.8,0.2]).predict(prediction_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also add multiple ML wrappers and get joltml to use all of them in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, LinearRegression, Lasso\n",
    "\n",
    "experiment = Experiment(training_set, experiment_id=\"trial1\")\n",
    "y = experiment.add_models([Xgboost(),\n",
    "                           Sklearn(ElasticNet()),\n",
    "                           Sklearn(LinearRegression()),\n",
    "                           Sklearn(Lasso()),\n",
    "                           ]).regression(target_names=['MedHouseVal'],splits=[0.8, 0.2]).predict(prediction_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are 10 models fitted together in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet,\\\n",
    "LinearRegression, Ridge, RidgeCV, SGDRegressor, Lars, Lasso,\\\n",
    "LassoLars, ARDRegression\n",
    "\n",
    "experiment = Experiment(training_set, experiment_id=\"trial2\")\n",
    "y = experiment.add_models([Xgboost(),\n",
    "                           Sklearn(ElasticNet()),\n",
    "                           Sklearn(LinearRegression()),\n",
    "                           Sklearn(Ridge()),\n",
    "                           Sklearn(RidgeCV()),\n",
    "                           Sklearn(SGDRegressor()),\n",
    "                           Sklearn(Lars()),\n",
    "                           Sklearn(Lasso()),\n",
    "                           Sklearn(LassoLars()),\n",
    "                           Sklearn(ARDRegression()),\n",
    "                           ]).regression(target_names=['MedHouseVal'],splits=[0.8, 0.2]).predict(prediction_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough with Xgboost and sklearn? Let's try training a simple `torch` neural network on a simple dataset: a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joltml import Experiment, Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "a = 1\n",
    "b = 0.5\n",
    "\n",
    "X = torch.linspace(0,10,1000)\n",
    "y = a * X + b\n",
    "\n",
    "data = pd.DataFrame(zip(X,y),columns=['X','y'])\n",
    "training_set = data.iloc[:800]\n",
    "prediction_set = data.iloc[800:,:-1]\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = nn.Parameter(torch.randn(\n",
    "            1, dtype=torch.float), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.randn(\n",
    "            1, dtype=torch.float), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.a*x + self.b\n",
    "\n",
    "\n",
    "experiment = Experiment(training_set,experiment_id='linear_relation')\n",
    "y = experiment.add_models([Pytorch(RegressionModel())]).regression(\n",
    "    target_names=['y'], splits=[0.8, 0.2]).predict(prediction_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a `torch` neural network on the California housing dataset, and compute the MAE. This will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joltml import Experiment, Pytorch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from joltml.joltmeter import RegressionMetrics\n",
    "\n",
    "dataset = fetch_california_housing(as_frame=True)[\"frame\"]\n",
    "dataset = dataset.dropna()\n",
    "dataset.astype(np.float32)\n",
    "dataset = dataset.sample(frac=1)\n",
    "training_size = int(0.9*len(dataset))\n",
    "training_set = dataset[:training_size]\n",
    "prediction_set = dataset.iloc[training_size:,:-1]\n",
    "prediction_y = dataset.iloc[training_size:,-1]\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(training_set.shape[1]-1, 24),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(24, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 1)\n",
    ")\n",
    "\n",
    "experiment = Experiment(training_set)\n",
    "y = experiment.add_models([Pytorch(model)]).regression(\n",
    "    target_names=['MedHouseVal'], splits=[0.8, 0.2]).predict(prediction_set)\n",
    "print(RegressionMetrics.mean_absolute_error.evaluate(y[0].detach().numpy(),prediction_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "# Logging is done implicitly\n",
    "# https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset\n",
    "dataset = load_diabetes(as_frame=True)[\"frame\"]\n",
    "dataset = dataset.dropna()\n",
    "training_set = dataset[:350]\n",
    "prediction_set = dataset.iloc[350:,:-1]\n",
    "\n",
    "experiment = Experiment(training_set,experiment_id='diabetes_1')\n",
    "experiment.add_models([Xgboost(n_estimators=400)])\n",
    "experiment.regression(target_names=['target'],splits=0.2)\n",
    "# Write results and default evaluation metrics\n",
    "y = experiment.predict(prediction_set)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've been doing regression tasks. Let's do classification tasks: the multi-class Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from joltml import Experiment, Xgboost\n",
    "from joltml.joltmeter import ClassificationMulticlassMetrics\n",
    "\n",
    "# Logging is done implicitly\n",
    "# https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset\n",
    "dataset = load_iris(as_frame=True)[\"frame\"]\n",
    "dataset = dataset.dropna()\n",
    "dataset = dataset.sample(frac=1)\n",
    "training_set = dataset[:100]\n",
    "prediction_set = dataset.iloc[100:,:-1]\n",
    "prediction_targets = dataset.iloc[100:,-1]\n",
    "\n",
    "experiment = Experiment(training_set,experiment_id='iris_1')\n",
    "experiment.add_models([Xgboost(objective='multi:softprob', num_class=3)])\n",
    "y = experiment.classification(target_names=['target'],splits=0.2,metrics=[ClassificationMulticlassMetrics.precision]).predict(prediction_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`joltml` applies `optuna` to perform hyperparameter optimization. Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joltml import Experiment, Xgboost, Sklearn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "# Logging is done implicitly\n",
    "dataset = fetch_california_housing(as_frame=True)[\"frame\"]\n",
    "dataset = dataset.dropna()\n",
    "training_set = dataset[:16000]\n",
    "prediction_set = dataset.iloc[16000:,:-1]\n",
    "\n",
    "params = {\n",
    "    'booster': {'type': 'categorical', 'values': ['gbtree', 'gblinear', 'dart']},\n",
    "    'lambda': {'type': 'float', 'minimum': 1e-8, 'maximum': 1.0},\n",
    "    'alpha': {'type': 'float', 'minimum': 1e-8, 'maximum': 1.0},\n",
    "}\n",
    "\n",
    "experiment = Experiment(training_set, experiment_id='trial1')\n",
    "experiment.add_models([Xgboost()])\n",
    "experiment.regression_optimize(\n",
    "    target_names=['MedHouseVal'], splits=0.2, n_trials=10, params=params)\n",
    "# Write results and default evaluation metrics\n",
    "y = experiment.predict(prediction_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
